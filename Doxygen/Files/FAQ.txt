// \file FAQ.txt
/*! 
\page FAQ.txt Frequently Asked Questions (FAQ)
\n
==========\n
Q1: Which file should I use to run the framework?\n
A1: Use main.py to run your experiment. You can use	multipleRuns.py to run 
the code in main.py in several threads, producing results of the format:\n
1-out.txt, 2-out.txt, 3-out.txt, ... \n
You can also import the project into Eclipse after installing the 'Pydev' package; see the 'Install' page of this documentation (http://acl.mit.edu/RLPy/index.html).\n
\n
\n
Q2: What does each line of output mean?\n
A2: See documentation in the \ref Interpreting_Output "Getting Started" section.\n
    "88825: E[0:01:23]-R[0:00:10]: Return=-1.00, Steps=56, Features = 174" means:\n
	88825: 			steps of learning\n
	E[0:01:23]: 		Elapsed time (s)\n
	R[0:00:10]: 		Remaining time (s)\n
	Return=-1.00: 	Sum of rewards for the last episode\n
	Steps=56: 		Number of steps for the last episode\n
	Features = 174 	Number of Features used for the last episode\n
\n
\n
Q3: My code is slow, how can I improve its speed?\n
A3: ProfileMe.py runs the code at main.py and generates a pictorial profile of the
	resulting running time in pdf format. Each node represents proportional time
	for finishing the function, proportional time spent within the function, and
	number of times it has been called. Nodes are color coded based on their time.
	You want to spend your time boosting the running time of nodes with the highest
	proportional time spent within them shown in parentheses. As an example you can
	view Profiling/Inverted_Pendulum-TabularSarsa.pdf
	It seems 'phi_sa' should be the place to improve the algorithm as 34.97% was spent
	within this function. 

Q4: My project does not work. Do I need to install packages?
A4: Please see the "Install" page or, if you cannot access it, read install.txt
    in the RLPy folder.\n
\n
\n
Q5: I used to plot my figures based on number of episodes, why do you prefer steps?\n
A5: The use of episode numbers does not provide accurate plots as the number of
	samples can vary within each episode. The use of steps gurantees that all
	methods saw exactly the same amount of data before being tested.\n
\n
\n
Q6: I have generated multipleRuns for various methods. How do I merge the results?\n
A6: Use mergeRuns.py. You should be able to call the mergeRuns.py with the committed
	example results. Lets assume you have the following directory structure:\n
				- Results/MyProject/\n
					- Domain-Algorithm-Representation1 \n
					- Domain-Algorithm-Representation2 \n
					- Domain-Algorithm-Representation3 \n
	Set the initial path for mergeRuns to "Results/MyProject". Also use the desired
	Y and X Axes from the following set: \n
		'Return': \t    \t  Sum of rewards\n
		'Features': \t  \t  Number of basis functions used\n
		'Steps': \t \t  \t  Length of the episode\n
		'Terminal': \t  \t  Did the episode finish due to reaching a terminal state\n
		 					or because the episode cap was reached.\n
		'Learning Steps':\t Number of interactions between the agent and domain\n
		'Time(s)': \t   \t  Clock Time in number of seconds\n
		'Episodes': \t  \t  Number of episodes finished between the agent and the domain \n
*/
